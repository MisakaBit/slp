# 2 正则表达式，文本标准化，编辑距离
> User: I am unhappy.  
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY  
User: I need some help, that much seems certain.  
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP  
User: Perhaps I could learn to get along with my mother.  
ELIZA: TELL ME MORE ABOUT YOUR FAMILY  
User: My mother takes care of me.  
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU  
User: My father.  
ELIZA: YOUR FATHER  
User: You are like my father in some ways.  

用户：我不开心  
ELIZA：你是否认为来到这里可以帮助你解决不开心的问题  
用户：我的确需要帮助  
ELIZA：得到帮助对你有什么用  
用户：或许我可以学会如何与母亲相处  
ELIZA：讲一讲你的家庭吧  
用户：我的母亲照顾我  
ELIZA：还有别人照顾你吗  
用户：我的父亲  
ELIZA：你的父亲  
用户：你有些方面像我的父亲  

> The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. Eliza’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA’s abilities even after the program’s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion.

上述对话来自 ELIZA，一个早期的自然语言处理系统，能够模仿罗杰式心理医师与用户进行有限的的对话。ELIZA 的逻辑简单的令人惊讶，它使用模式匹配来识别 “我需要X” 这样的句子并转化出与之相对应的 “X对你有什么用？”。这项简单的技术在这个领域取得了成功因为 ELIZA 实际上不需要知道任何模仿罗杰式心理医师的有关知识。正如 Weizenbaum 所指出的，这是听众完全不了解这个世界却能进行交流的少数几种对话类型之一。ELIZA 对人类对话的模仿异常成功：许多与 ELIZA 互动过的人都相信它真的能理解他们及提出的问题，甚至当程序的原理已经被解释出来，许多人依然相信 ELIZA 的能力，即便在今天这样的聊天机器人还是一个有趣的话题。

> Of course modern conversational agents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for which they rely on a much more sophisticated understanding of the user’s intent, as we will see in Chapter 24. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural language processing.

> We’ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming “I need X” in Eliza above, to defining strings like $199 or $24.99 for extracting tables of prices from a document.

> We’ll then turn to a set of tasks collectively called text normalization, in which  regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock ’n’ roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate I’m into the two words I and am. For processing tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.Some languages, like Japanese, don’t have spaces between words, so word tokenization becomes more difficult. 

> Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points. 

> Finally, we’ll need to compare words and other strings. We’ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution.